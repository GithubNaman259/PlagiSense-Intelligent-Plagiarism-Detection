# ðŸ“˜ PlagiSense: Intelligent Plagiarism Detection Using DSA + NLP

**PlagiSense** is a smart plagiarism detection system that combines classic Data Structures & Algorithms (DSA) with Natural Language Processing (NLP). It detects both **exact word-level matches** and **meaning-level paraphrasing** using a dual-layered approach:
- **Rabin-Karp Algorithm** for exact match detection
- **Sentence Embedding** for semantic similarity

---

## ðŸš€ Features

- âœ… Detects both direct copy-paste and paraphrased content
- ðŸ§  Uses **Rabin-Karp** for fast exact matching of word sequences
- ðŸ§  Applies **Sentence Embedding** to catch semantic similarities
- ðŸ“ Generates a detailed plagiarism report (`report.txt`)
- ðŸŽ¯ Calculates a plagiarism risk score and categorizes it

---

## ðŸ“ File Structure

```text
PlagiSense/
â”œâ”€â”€ main.py # Main driver script
â”œâ”€â”€ utility.py # Contains all logic: preprocessing, matching, scoring
â”œâ”€â”€ sample1.txt # First input text file
â”œâ”€â”€ sample2.txt # Second input text file
â”œâ”€â”€ report.txt # Auto-generated plagiarism report
â””â”€â”€ README.md # Project documentation
```

---

## ðŸ” Algorithm Overview

### 1. Rabin-Karp Algorithm (Exact Matching)
- A rolling hash-based string matching algorithm
- Efficiently detects **identical 5-word phrases** (5-grams) between two texts files

### 2. Sentence Embeddings (Semantic Matching)
Sentence Embeddings represent sentences as **numerical vectors** in high-dimensional space based on their meaning. Similar meanings result in closer vectors.

> ðŸ“Œ **Example**:
> - "The cat is on the mat" and  
> - "A feline lies on the rug"  
> Both mean similar things, so their embeddings will have **high cosine similarity**, even if no words are exactly the same.

We use the `MiniLM` model from `sentence-transformers` to compare such 5-word sequences for **semantic overlap**.

---

## ðŸ§ª Running the Project on Your Machine

Follow these steps to run PlagiSense locally:

### 1. Clone or download the repository  
Make sure all the files (`main.py`, `utility.py`, `sample1.txt`, etc.) are in the same directory.

### 2. Install required packages

Open terminal or command prompt and run:

```bash
pip install nltk sentence-transformers
```

### 3. Download necessary NLTK resources

```bash
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```
Or you can just run `main.py` once â€” it will automatically download them the first time.

### 4. Add Input Files

Replace the content of `sample1.txt` and `sample2.txt` with your own content to test.

### 5. Run the script

Run `main.py`

### 6. Check Output

Results will be printed in the **terminal** and saved to `report.txt`, including:

- Exact matches (Rabin-Karp)
- Semantic matches (Sentence Embedding)
- Plagiarism Score
- Risk Level (High/Moderate/Low)

---









